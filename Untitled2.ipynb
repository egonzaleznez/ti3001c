{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66caaf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/enriquegon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/enriquegon/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.1.0-cp310-cp310-macosx_10_9_x86_64.whl (37 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import tkinter as tk\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "\n",
    "from nltk.corpus import words\n",
    "from difflib import get_close_matches\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5d0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.geometry('800x500')\n",
    "        self.root.title('Spell Checker')\n",
    "\n",
    "        # Use grid geometry manager for better control\n",
    "        self.root.grid_rowconfigure(0, weight=1)\n",
    "        self.root.grid_columnconfigure(0, weight=1)\n",
    "        self.root.grid_columnconfigure(1, weight=0)\n",
    "\n",
    "        self.text_frame = tk.Frame(self.root)\n",
    "        self.text_frame.grid(row=0, column=0, sticky='nsew')\n",
    "\n",
    "        self.text = ScrolledText(self.text_frame, font=(\"Garamond\", 14))\n",
    "        self.text.pack(fill=tk.BOTH, expand=True)\n",
    "        self.text.bind('<KeyRelease>', self.checkSpelling)\n",
    "\n",
    "        self.suggestions_frame = tk.Frame(self.root, width=300)\n",
    "        self.suggestions_frame.grid(row=0, column=1, sticky='ns')\n",
    "        self.suggestions_frame.grid_propagate(False)  # Prevent frame from resizing\n",
    "\n",
    "        self.suggestions_label = tk.Label(self.suggestions_frame, text=\"Suggestions\", font=(\"Garamond\", 14))\n",
    "        self.suggestions_label.pack()\n",
    "\n",
    "        self.suggestions_list = tk.Listbox(self.suggestions_frame, font=(\"Garamond\", 12))\n",
    "        self.suggestions_list.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        self.old_spaces = 0\n",
    "\n",
    "        self.root.mainloop()\n",
    "\n",
    "    def checkSpelling(self, event):\n",
    "        content = self.text.get('1.0', tk.END)\n",
    "\n",
    "        space_count = content.count(' ')\n",
    "        if space_count != self.old_spaces:\n",
    "            self.old_spaces = space_count\n",
    "            for tag in self.text.tag_names():\n",
    "                self.text.tag_delete(tag)\n",
    "\n",
    "            self.suggestions_list.delete(0, tk.END)\n",
    "\n",
    "            \n",
    "            content = contractions.fix(content)\n",
    "            words_list = content.split()\n",
    "\n",
    "            for i, word in enumerate(words_list):\n",
    "                clean_word = re.sub(r'[^\\w]', '', word.lower())\n",
    "                print(f\"Word: {word}, Clean Word: {clean_word}\") # Debugging\n",
    "                if clean_word and clean_word not in words.words():\n",
    "                    start_idx = '1.0'\n",
    "                    while True:\n",
    "                        start_idx = self.text.search(word, start_idx, stopindex=tk.END)\n",
    "                        if not start_idx:\n",
    "                            break\n",
    "                        end_idx = f\"{start_idx}+{len(word)}c\"\n",
    "                        self.text.tag_add(word, start_idx, end_idx)\n",
    "                        self.text.tag_config(word, foreground='red')\n",
    "                        start_idx = end_idx\n",
    "\n",
    "                    suggestions = get_close_matches(clean_word, words.words(), n=5, cutoff=0.8)\n",
    "                    \n",
    "                    \n",
    "                    if suggestions:\n",
    "                        self.suggestions_list.insert(tk.END, f\"{word}: {', '.join(suggestions)}\")\n",
    "                        print(f\"Suggestions = {word}: {', '.join(suggestions)}\") # Debugging\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fb279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: are, Clean Word: are\n",
      "Word: are, Clean Word: are\n",
      "Word: you, Clean Word: you\n",
      "Word: are, Clean Word: are\n",
      "Word: you, Clean Word: you\n",
      "Word: are, Clean Word: are\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: ca, Clean Word: ca\n",
      "Word: I, Clean Word: i\n",
      "Word: ca, Clean Word: ca\n",
      "Word: do, Clean Word: do\n",
      "Word: I, Clean Word: i\n",
      "Word: ca, Clean Word: ca\n",
      "Word: do, Clean Word: do\n",
      "Word: my, Clean Word: my\n",
      "Word: I, Clean Word: i\n",
      "Word: ca, Clean Word: ca\n",
      "Word: do, Clean Word: do\n",
      "Word: my, Clean Word: my\n",
      "Word: homeworkd, Clean Word: homeworkd\n",
      "Suggestions = homeworkd: homework, homeworker, homewort, homeward\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: a, Clean Word: a\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: a, Clean Word: a\n",
      "Word: youn, Clean Word: youn\n",
      "Suggestions = youn: yourn, young, young, you, you\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: a, Clean Word: a\n",
      "Word: youn, Clean Word: youn\n",
      "Suggestions = youn: yourn, young, young, you, you\n",
      "Word: I, Clean Word: i\n",
      "Word: am, Clean Word: am\n",
      "Word: a, Clean Word: a\n",
      "Word: young, Clean Word: young\n",
      "Word: I'mm, Clean Word: imm\n",
      "Suggestions = I'mm: immi, imam\n",
      "Word: a, Clean Word: a\n",
      "Word: youn, Clean Word: youn\n",
      "Suggestions = youn: yourn, young, young, you, you\n",
      "Word: boy, Clean Word: boy\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: I, Clean Word: i\n",
      "Word: cann-t, Clean Word: cannt\n",
      "Suggestions = cann-t: cannot, cannet, cant, candent, scant\n",
      "Word: I, Clean Word: i\n",
      "Word: cann-t, Clean Word: cannt\n",
      "Suggestions = cann-t: cannot, cannet, cant, candent, scant\n",
      "Word: I, Clean Word: i\n",
      "Word: cann/t, Clean Word: cannt\n",
      "Suggestions = cann/t: cannot, cannet, cant, candent, scant\n",
      "Word: I, Clean Word: i\n",
      "Word: cann/t, Clean Word: cannt\n",
      "Suggestions = cann/t: cannot, cannet, cant, candent, scant\n",
      "Word: I, Clean Word: i\n",
      "Word: cann't, Clean Word: cannt\n",
      "Suggestions = cann't: cannot, cannet, cant, candent, scant\n",
      "Word: I, Clean Word: i\n",
      "Word: cann't, Clean Word: cannt\n",
      "Suggestions = cann't: cannot, cannet, cant, candent, scant\n",
      "Word: dance, Clean Word: dance\n",
      "Word: I, Clean Word: i\n",
      "Word: cann't, Clean Word: cannt\n",
      "Suggestions = cann't: cannot, cannet, cant, candent, scant\n",
      "Word: dance, Clean Word: dance\n",
      "Word: any, Clean Word: any\n"
     ]
    }
   ],
   "source": [
    "SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f84ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto: j'aime manger des pommes.  Está en francés.\n",
      "El texto: me encanta comer pizza los fines de semana.  Está en español.\n",
      "El texto: el perro juega en el parque.  Está en español.\n",
      "El texto: i love to travel around the world.  Está en inglés.\n",
      "El texto: the cat is sleeping on the couch.  Está en inglés.\n",
      "El texto ich mag bücher lesen am wochenende. No se pudo determinar el idioma con claridad.\n"
     ]
    }
   ],
   "source": [
    "# Listas de palabras comunes en español, inglés y francés\n",
    "palabras_comunes_espanol = [\n",
    "    \"el\", \"la\", \"es\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\", \"haber\", \"por\",\n",
    "    \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\", \"todo\", \"pero\", \"más\", \"o\", \"poder\"\n",
    "]\n",
    "\n",
    "palabras_comunes_ingles = [\n",
    "    \"the\", \"is\", \"in\", \"and\", \"to\", \"of\", \"a\", \"that\", \"it\", \"for\", \"with\", \"as\", \"on\", \"at\",\n",
    "    \"by\", \"from\", \"about\", \"this\", \"but\", \"they\", \"be\", \"have\", \"not\", \"are\", \"or\", \"was\", \"so\"\n",
    "]\n",
    "\n",
    "palabras_comunes_frances = [\n",
    "    \"le\", \"la\", \"et\", \"de\", \"les\", \"des\", \"que\", \"un\", \"être\", \"ce\", \"dans\", \"il\", \"elle\", \"pour\",\n",
    "    \"avec\", \"ne\", \"pas\", \"par\", \"sur\", \"plus\", \"au\", \"se\", \"en\", \"avoir\", \"son\", \"mais\", \"nous\"\n",
    "]\n",
    "\n",
    "def detectar_idioma(texto):\n",
    "    texto = texto.lower()  # Convertir a minúsculas\n",
    "    palabras = texto.split()  # Dividir el texto en palabras\n",
    "\n",
    "    # Inicializar contadores para cada idioma\n",
    "    contador_espanol = 0\n",
    "    contador_ingles = 0\n",
    "    contador_frances = 0\n",
    "\n",
    "    # Contamos palabras comunes en cada idioma\n",
    "    for palabra in palabras:\n",
    "        if palabra in palabras_comunes_espanol:\n",
    "            contador_espanol += 1\n",
    "        elif palabra in palabras_comunes_ingles:\n",
    "            contador_ingles += 1\n",
    "        elif palabra in palabras_comunes_frances:\n",
    "            contador_frances += 1\n",
    "\n",
    "    # Determinamos el idioma basado en los contadores\n",
    "    if contador_espanol > contador_ingles and contador_espanol > contador_frances:\n",
    "        print(\"El texto:\" + texto + \"  Está en español.\")\n",
    "    elif contador_ingles > contador_espanol and contador_ingles > contador_frances:\n",
    "        print(\"El texto:\" + texto + \"  Está en inglés.\")\n",
    "    elif contador_frances > contador_espanol and contador_frances > contador_ingles:\n",
    "        print(\"El texto:\" + texto + \"  Está en francés.\")\n",
    "    else:\n",
    "        print(\"El texto\" + texto + \" No se pudo determinar el idioma con claridad.\")\n",
    "\n",
    "# Ejemplos\n",
    "texto = \" J'aime manger des pommes.\"\n",
    "detectar_idioma(texto)\n",
    "\n",
    "texto = \" Me encanta comer pizza los fines de semana.\"\n",
    "detectar_idioma(texto)\n",
    "\n",
    "texto = \" El perro juega en el parque.\"\n",
    "detectar_idioma(texto)\n",
    "\n",
    "\n",
    "texto = \" I love to travel around the world.\"\n",
    "detectar_idioma(texto)\n",
    "\n",
    "texto = \" The cat is sleeping on the couch.\"\n",
    "detectar_idioma(texto)\n",
    "\n",
    "texto=\" Ich mag Bücher lesen am Wochenende.\"\n",
    "detectar_idioma(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3468f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El textocanto alegremente mientras voy caminando No se pudo determinar el idioma con claridad.\n"
     ]
    }
   ],
   "source": [
    "texto=\"Canto alegremente mientras voy caminando\"\n",
    "detectar_idioma(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6abd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El textocanto alegremente mientras camino No se pudo determinar el idioma con claridad.\n"
     ]
    }
   ],
   "source": [
    "texto=\"Canto alegremente mientras camino\"\n",
    "detectar_idioma(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff1d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El textoyou sing very loudly No se pudo determinar el idioma con claridad.\n"
     ]
    }
   ],
   "source": [
    "texto=\"you sing very loudly\"\n",
    "detectar_idioma(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932d59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68632cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycountry\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Universidad Panamericana\n",
    "\n",
    "Natural Language Processing (NLP)\n",
    "Prof. Enrique González Núñez\n",
    "\n",
    "1st Period Exam - Language Detection\n",
    "\n",
    "Javier Alejandro Rangel Murillo\n",
    "0256158\n",
    "'''\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import pycountry\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Expanded language profiles with character frequency and common words\n",
    "language_profiles = {\n",
    "    'en': {\n",
    "        'char_freq': Counter(\"etaoinshrdlcumwfgypbvkjxqz\"),\n",
    "        'common_words': stopwords.words('english')\n",
    "    },\n",
    "    'es': {\n",
    "        'char_freq': Counter(\"eaosrnidlcutmqbpvhgyfzxj\"),\n",
    "        'common_words': stopwords.words('spanish')\n",
    "    },\n",
    "    'fr': {\n",
    "        'char_freq': Counter(\"esaitnrulodcmpvbqfgjhxykz\"),\n",
    "        'common_words': stopwords.words('french')\n",
    "    },\n",
    "    'de': {\n",
    "        'char_freq': Counter(\"enisratdhlcumobwgfvkzpjyxq\"),\n",
    "        'common_words': stopwords.words('german')\n",
    "    },\n",
    "    'it': {\n",
    "        'char_freq': Counter(\"eaionlrtscudmpghvfbqz\"),\n",
    "        'common_words': stopwords.words('italian')\n",
    "    },\n",
    "    'pt': {\n",
    "        'char_freq': Counter(\"aeosridmntculpvghqbfjxz\"),\n",
    "        'common_words': stopwords.words('portuguese')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Preprocess text (lowercase, remove special characters)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zñáéíóúüäöß]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to compare character frequency profiles\n",
    "def compare_char_freq(input_text, profile_freq):\n",
    "    input_freq = Counter(preprocess_text(input_text))\n",
    "    score = 0\n",
    "    total_chars = sum(input_freq.values())\n",
    "    if total_chars > 0:\n",
    "        for char, count in input_freq.items():\n",
    "            if char in profile_freq:\n",
    "                profile_percentage = profile_freq[char] / sum(profile_freq.values())\n",
    "                input_percentage = count / total_chars\n",
    "                score += 1 - abs(profile_percentage - input_percentage)\n",
    "    return score\n",
    "\n",
    "# Function to compare common word usage using NLTK tokenization\n",
    "def compare_common_words(input_text, common_words):\n",
    "    words = preprocess_text(input_text).split()\n",
    "    # Calculate how many words match the stopwords for the given language\n",
    "    word_count = sum(1 for word in words if word in common_words)\n",
    "    \n",
    "    return word_count\n",
    "\n",
    "# Function to compare N-gram profiles (trigrams as an example)\n",
    "def compare_ngrams(input_text, profile_freq):\n",
    "    input_text = preprocess_text(input_text)\n",
    "    input_trigrams = [input_text[i:i+3] for i in range(len(input_text) - 2)]\n",
    "    profile_trigrams = [profile_freq[i:i+3] for i in range(len(profile_freq) - 2)]\n",
    "    \n",
    "    trigram_count = sum(1 for trigram in input_trigrams if trigram in profile_trigrams)\n",
    "    return trigram_count\n",
    "\n",
    "# Main language detection function\n",
    "def detect_language_custom(text):\n",
    "    scores = {}\n",
    "    \n",
    "    for lang, profile in language_profiles.items():\n",
    "        # Calculate the character frequency, common word similarity, and N-gram similarity scores\n",
    "        char_score = compare_char_freq(text, profile['char_freq'])\n",
    "        word_score = compare_common_words(text, profile['common_words'])\n",
    "        total_score = char_score + word_score\n",
    "        \n",
    "        scores[lang] = total_score\n",
    "    \n",
    "    detected_language = max(scores, key=scores.get)\n",
    "    \n",
    "    return detected_language, scores\n",
    "\n",
    "# Function to map language codes to language names using pycountry\n",
    "def langCodetoName(code):\n",
    "    try:\n",
    "        language = pycountry.languages.get(alpha_2=code)\n",
    "        if language is None:\n",
    "            language = pycountry.languages.get(alpha_3=code)\n",
    "        return language.name if language else \"Unknown\"\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Function to read the contents of a file\n",
    "def importtxt(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Function to display the detection results\n",
    "def display_results(detected_language, scores):\n",
    "    print(\"\\nDetected language: \", langCodetoName(detected_language))\n",
    "    print(\"Language detection scores:\")\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for lang, score in sorted_scores[:3]:\n",
    "        print(langCodetoName(lang), \" - \", score)\n",
    "\n",
    "# Main program loop\n",
    "while True:\n",
    "    print(\"\\nCustom Language Detection\")\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Menu:\")\n",
    "    print(\"1. Input text in console\")\n",
    "    print(\"2. Input text from file\")\n",
    "    print(\"3. Exit\")\n",
    "    choice = input(\"Enter your choice: \")\n",
    "\n",
    "    if choice == '1':\n",
    "        # Detect language from user input\n",
    "        text = input(\"Enter the text: \")\n",
    "        detected_language, scores = detect_language_custom(text)\n",
    "        display_results(detected_language, scores)\n",
    "    elif choice == '2':\n",
    "        # Detect language from file input\n",
    "        file = input(\"Enter the filename: \") # Example: 'lincoln_speech.txt'\n",
    "        try:\n",
    "            text = importtxt(file)\n",
    "            detected_language, scores = detect_language_custom(text)\n",
    "            display_results(detected_language, scores)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Please try again.\")\n",
    "    elif choice == '3':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice. Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdba88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
